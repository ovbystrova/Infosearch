{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа\n",
    "## Ранжирование с помощью ML\n",
    "\n",
    "\n",
    "![](https://avatars.mds.yandex.net/get-research/1677227/2a00000168a82fc9b0eac19e430b8454a656/orig)\n",
    "\n",
    "\n",
    "Одна из отличительных особенностей задачи ранжирования от классических задач машинного обучения заключается в том, что качество результата зависит не от предсказанных оценок релевантности, а от порядка следования документов в рамках конкретного запроса, т.е. важно не абсолютное значение релевантности (его достаточно трудно формализовать в виде числа), а то, более или менее релевантен документ, относительно других документов.\n",
    "\n",
    "### Подходы к решению задачи ранжирования\n",
    "Существуют 3 основных подхода к ранжированию, различие между которыми заключается в том, на какую функцию потерь они опираются:\n",
    "  \n",
    "1. **Поточечный подход (pointwise)**. В этом подходе предполагается, что каждой паре запрос-документ поставлена в соответствие численная оценка. Задача обучения ранжированию сводится к построению регрессии: для каждой отдельной пары запрос-документ необходимо предсказать её оценку.\n",
    "\n",
    "2. **Попарный подход (pairwise)**. В таком подходе обучение ранжированию сводится к построению бинарного классификатора, которому на вход поступают два документа, соответствующих одному и тому же запросу, и требуется определить, какой из них лучше. Другими словами, функция потерь штрафует модель, если отранжированная этой моделью пара документов оказалась в неправильном порядке.\n",
    "\n",
    "3. **Списочный подход (listwise)**. Его суть заключается в построении модели, на вход которой поступают сразу все документы, соответствующие запросу, а на выходе получается их перестановка.\n",
    "\n",
    "\n",
    "Будем использовать самый простой подход - поточечный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества\n",
    "\n",
    "Для оценивания качества ранжирования найденных документов в поиске традиционно используется метрика *DCG* ([Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)) и ее нормализованный вариант — *nDCG*, всегда принимающий значения от 0 до 1.\n",
    "\n",
    "Для одного запроса DCG считается следующим образом:\n",
    "$$ DCG(Q) = \\sum_{i=1}^{numpos}\\frac{(2^{rel_i} - 1)}{\\log_2(i+1)}, $$\n",
    "где\n",
    ">$numpos$ — количество документов в поисковой выдаче, среди которых мы оценимваем качество (например, в предудыщих заданиях *num_pos* был равен 5)  \n",
    "$rel_i$ — оценка релевантности документа, находящегося на i-той позиции   \n",
    "   \n",
    "\n",
    "Нормализованный вариант *nDCG* получается делением *DCG* на максимальное из его значений:\n",
    "\n",
    "$$nDCG = \\frac{DCG}{IDCG} \\in [0, 1].$$\n",
    "> *IDCG* — наибольшее из возможных значение *DCG* \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Чтобы оценить значение *nDCG* на выборке $Queries$ ($nDCG_{Queries}$) размера $N$, необходимо усреднить значение *nDCG* по всем запросам  выборки:\n",
    "$$nDCG_{Queries} = \\frac{1}{N}\\sum_{q \\in Queries}nDCG(q).$$\n",
    "\n",
    "Пример реализации метрик ранжирование на python можно найти [здесь](https://gist.github.com/mblondel/7337391)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Погнали\n",
    "###  **Задача: предсказать оценку релевантности для запросов тестового датасета**\n",
    "\n",
    "\n",
    "Мы будем работать на данных с конкурса [Интернет-математика 2009](https://academy.yandex.ru/events/data_analysis/grant2009/). По ссылке можно прочитать описание данных.      \n",
    "\n",
    "Данные\n",
    "> Данные разбиты на две выборки – обучающая выборка imat2009_learning.txt с известными оценками близости запроса и документа и тестовая выборка с неизвестными близостями imat2009_test.txt  \n",
    "\n",
    "Обучающая выборка\n",
    "> Данные для обучения содержат **97 290 строк**, которые соответствуют **9 124 запросам**  \n",
    "Каждая строка соответствует паре «запрос-документ»    \n",
    "\n",
    "Признаки\n",
    ">Каждой паре «запрос-документ» соответствуют значения **245 признаков**. Формат хранения feat_num:value. Если значение признака равно 0, то он опускается.     \n",
    "В комментариях в конце каждой строки указан **идентификатор запроса**.   \n",
    "Файл с обучающей выборкой содержит **оценку релевантности**, значения из диапазона **[0, 4]** (4 – «высокая релевантность», 0 – «нерелевантно»).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "file_learning = 'imat2009-datasets/imat2009_learning.txt'\n",
    "\n",
    "with open(file_learning) as f:\n",
    "    train_data = f.readlines()\n",
    "    \n",
    "    \n",
    "# LOAD TEST DATA\n",
    "file_test = 'imat2009-datasets/imat2009_test.txt'\n",
    "\n",
    "with open(file_test) as f:\n",
    "    test_data = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97290, 115643)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура данных следующая - первый элемент в строке - это оценка близости запроса и документа, дальше идут признаки документа, а последний элемент строки - это id запроса:\n",
    "\n",
    "> RELEVANCE      feature:value feature:value ... feature:value     # QUERY_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 1:0.000023 7:0.704953 8:0.550315 9:0.032294 11:0.712631 14:0.015686 15:0.137255 16:0.302576 17:1.000000 18:0.996078 22:1.000000 23:1.000000 24:1.000000 27:0.700000 28:0.587629 29:0.999881 30:0.032294 34:0.000023 36:0.431373 37:0.002247 38:0.054902 41:1.000000 46:0.002247 50:0.032294 51:0.325613 52:0.056641 53:0.820677 54:0.388235 55:0.450980 56:0.312547 57:0.004672 59:1.000000 61:0.000023 65:1.000000 68:0.712195 69:0.001400 70:1.000000 71:0.001013 73:0.709459 74:0.560784 76:0.142857 77:0.360800 78:1.000000 79:1.000000 80:1.000000 82:0.000023 83:1.000000 85:0.996078 86:0.070588 87:1.000000 88:0.999797 92:1.000000 93:0.714286 95:0.039216 97:0.000023 98:0.356490 99:0.165041 102:1.000000 103:1.000000 104:1.000000 105:0.486275 108:0.152941 120:0.996078 121:0.676507 122:0.032294 126:0.712980 128:0.121569 129:0.609261 132:1.000000 134:0.109804 135:0.030535 140:0.002247 142:0.698039 144:0.248111 145:0.356490 146:1.000000 147:0.498039 148:0.125490 150:0.704953 151:1.000000 152:0.098039 154:0.676507 156:0.066667 157:0.001470 160:0.101961 162:0.302576 165:0.843126 166:0.400000 167:0.019608 168:0.056641 171:1.000000 172:0.857143 177:0.285714 178:0.588235 179:0.820677 180:0.032294 181:0.196491 182:0.729730 185:0.756863 192:1.000000 193:1.000000 197:0.032294 202:0.310127 203:0.001186 205:1.000000 206:0.999835 209:0.291145 210:0.980392 211:0.960784 212:0.032294 213:0.000023 214:1.000000 216:0.999998 217:0.146074 219:0.300000 222:0.666667 224:0.145098 227:0.007089 228:1.000000 229:1.000000 230:0.032294 232:1.000000 233:0.494217 236:0.032749 243:0.000023 244:1.000000 245:0.000023 # 3382\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В test_data все оценки релевантности скрыты, поскольку этот набор данных использовался для проверки качества работы алгоритма в конкурсе. Нам эти данные не нужны, дальше работаем только с **train_data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки качества будущей модели надо разбить обучающую выборку на обучение и валидацию в соотношении 70 / 30\n",
    "\n",
    "Внимание: разбивать необходимо **множество запросов QUERY_ID**, а не строчки датасета, чтобы в выборке находилась вся информация по запросу\n",
    "\n",
    "Для этого вам надо:\n",
    "1. собрать все запросы для каждого QUERY_ID\n",
    "\n",
    "```\n",
    "{\n",
    "query_id : [\n",
    "    RELEVANCE feature:value ... feature:value,\n",
    "    ...\n",
    "],\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "При этом я бы сразу собирала не сами данные, а номер строки в матрице данных\n",
    "```\n",
    "{\n",
    "query_id : [\n",
    "    line_num, line_num, ... line_num\n",
    "],\n",
    "...\n",
    "}\n",
    "```\n",
    "2. собрать матрицу данных, размер вектора равен числу признаков = 245\n",
    "```\n",
    "data = np.zeros((len(train_data), feats_num), dtype=np.float32) \n",
    "```\n",
    "\n",
    "3. собрать вектор с оценками релевантности, его размер равен размеру train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "labels = [] \n",
    "queries_lines_info = defaultdict(list) \n",
    "\n",
    "data = np.zeros((len(train_data), 245), dtype=np.float32) \n",
    "\n",
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert data.shape == (len(train_data), 245)\n",
    "assert len(queries.keys()) == 9124\n",
    "assert len(labels) == len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим запросы из *queries_lines_info.keys()* на обучающую *train_queries_ids* и валидационную выборки *test_queries_ids* (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert len(train_queries_ids) / (len(train_queries_ids) + len(test_queries_ids)) == 0.6999123191582639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Теперь у нас есть:  \n",
    " 1) айдишники запросов для обучения и валидации **queries_id_train, queries_id_test**   \n",
    " 2) матрица данных **data**   \n",
    " 3) словарь **queries** с информацией о том, какие строчки в этой матрице соответствуют какому айдишнику  \n",
    " \n",
    " С помощью этих данных разделите матрицу data на матрицы **X_train, y_train, X_test, y_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [3, 3, 3]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# изи пизи способ получить несколько строк матрицы по их id данные матрицы\n",
    "data_example = np.array(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [1, 1, 1],\n",
    "        [2, 2, 2],\n",
    "        [3, 3, 3]\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_example[[0, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиваем номера строк исходной матрицы на train и test\n",
    "\n",
    "train_queries_lines_info = []\n",
    "test_queries_lines_info = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "train_queries_lines_info = np.array(train_queries_lines_info)\n",
    "test_queries_lines_info = np.array(test_queries_lines_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape == (68418, 245) \n",
    "assert len(y_train) == 68418"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поздравляю, если вы все сделали до этого моменты, вы восхитительны! \n",
    "\n",
    "Данные готовы, можно заряжать модели                                                           \n",
    "Для оценивания качества моделей используйте метрику nDCG, реализованную ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "\n",
    "test_lines = np.array(test_lines)\n",
    "\n",
    "\n",
    "def get_nDCG_score(queries, queries_lines_info, test_queries_lines_info, labels_true, labels_predicted):\n",
    "    nDCG_scores = [] # nDCG по каждому запросу\n",
    "    \n",
    "    for query in queries:\n",
    "        \n",
    "        query_lines = queries_lines_info[query]\n",
    "        query_lines_in_testdata = [np.where(test_queries_lines_info==line)[0][0] for line in query_lines]\n",
    "        \n",
    "        query_labels_true = labels[query_lines]\n",
    "        query_labels_pred = labels_predicted[query_lines_in_testdata]\n",
    "        \n",
    "        nDCG = metrics.ndcg_score(query_labels_true, query_labels_pred, k=10)\n",
    "        nDCG_scores.append(nDCG)\n",
    "        \n",
    "    nDCG_Queries = np.sum(nDCG_scores) / len(queries) # усредняем по всем запросам\n",
    "    return nDCG_Queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIT PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Воспользовавшись известными вам техниками построения линейной регрессии, обучите модель, предсказывающую оценку асессора\n",
    "\n",
    "``` from sklearn.linear_model import LinearRegression``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "lin_reg_y_pred = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "h = plt.hist(lr_y_pred, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем качество модели по метрике **nDCG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lin_reg = get_nDCG_score(\n",
    "    queries = q_id_test, \n",
    "    queries_lines_info = queries_lines_info, \n",
    "    test_queries_lines_info = test_queries_lines_info, \n",
    "    labels_true = y_test, \n",
    "    labels_predicted = lin_reg_y_pred\n",
    ")\n",
    " \n",
    "score_lin_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь решим эту задачу не как регрессию, а как классификацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ранжируем с RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ранжируем с XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ранжируем с LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
